# Azure DevOps Pipeline: Deploy Databricks SQL Scripts for Testing
# ---------------------------------------------------------------
# This pipeline uploads SQL scripts from the 'scripts/' folder to
# DBFS (Databricks File System), triggers a pre-configured Databricks
# Job to run them on a cluster, and waits for the result.
#
# Stage flow:
#   Scan         (all branches) - Security scan + SQLFluff lint/auto-fix (SparkSQL/Databricks)
#   Deploy       (all branches) - Upload scripts to DBFS (validation workspace)
#   Test         (all branches) - Run & validate Databricks Job (validation workspac
#   DeployToTest (main only)    - Upload scripts + run job on the TEST environment
#   Notify       (always)       - Post build result + logs link to Slack
#
# Authentication: Azure Managed Identity (self-hosted agent required).
# Prerequisites:
#   - A self-hosted Azure DevOps agent with a Managed Identity assigned.
#   - The Managed Identity must have the 'Contributor' role (or custom
#     Databricks role) on both Databricks workspaces.
#   - Databricks Jobs pre-configured to read scripts from their DBFS paths.
#   - Azure DevOps Service Connections of type 'Azure Resource Manager'
#     for both the validation and test workspaces.
#   - An Azure DevOps Environment named 'databricks-test' (used for optional
#     approval gates before deploying to the test environment).

trigger:
  branches:
    include:
      - main
      - develop

pr:
  branches:
    include:
      - main

variables:
  # --- Shared ---
  SCRIPTS_FOLDER: 'scripts'          # Source folder in the repo
  # --- Polling: max wait = MAX_POLL_RETRIES * POLL_INTERVAL_SECONDS ---
  MAX_POLL_RETRIES: '30'
  POLL_INTERVAL_SECONDS: '20'

  # --- Validation workspace (all branches: used by Deploy + Test stages) ---
  AZURE_SERVICE_CONNECTION: 'your-azure-service-connection'
  DATABRICKS_HOST: 'https://<your-validation-workspace>.azuredatabricks.net'
  DATABRICKS_JOB_ID: '0'             # Job ID in the validation workspace
  DBFS_SCRIPTS_PATH: 'dbfs:/mnt/scripts/sql'

  # --- Test environment (main branch only: used by DeployToTest stage) ---
  TEST_AZURE_SERVICE_CONNECTION: 'your-test-azure-service-connection'
  TEST_DATABRICKS_HOST: 'https://<your-test-workspace>.azuredatabricks.net'
  TEST_DATABRICKS_JOB_ID: '0'        # Job ID in the test workspace
  TEST_DBFS_SCRIPTS_PATH: 'dbfs:/mnt/scripts/sql'

  # --- Slack notifications ---
  # Store the webhook URL as a secret pipeline variable in Azure DevOps
  # (Pipelines > Edit > Variables) rather than hard-coding it here.
  # Name the secret variable 'SLACK_WEBHOOK_URL' and it will be picked up below.
  SLACK_CHANNEL: '#databricks-deployments'  # Display value only, channel is set on the webhook itself

# Self-hosted pool is required to use Azure Managed Identity.
# Replace 'Self-Hosted-MI-Pool' with your pool name.
pool:
  name: 'Self-Hosted-MI-Pool'

stages:

  # ------------------------------------------------------------------ #
  # Stage 1: Security scan + SQL linting (blocks deploy on failure)     #
  # ------------------------------------------------------------------ #
  # Tools used:                                                         #
  #   semgrep  - detects secrets, SQL injection, and dangerous patterns #
  #   sqlfluff - Databricks/SparkSQL dialect linter; auto-fixes style   #
  #              and best-practice issues, then re-lints to catch any   #
  #              problems that could not be fixed automatically.        #
  # ------------------------------------------------------------------ #
  - stage: Scan
    displayName: 'Security Scan + SQL Lint'
    jobs:
      - job: SecurityAndLint
        displayName: 'Semgrep + SQLFluff (SparkSQL)'
        steps:

          - checkout: self
            displayName: 'Checkout repository'

          - script: |
              set -euo pipefail
              echo "Installing scan tools..."
              pip install --quiet semgrep sqlfluff
            displayName: 'Install semgrep and sqlfluff'

          # ── Step 1: Semgrep security scan ──────────────────────────
          # Checks for:
          #   - Hardcoded secrets / passwords / API keys
          #   - SQL injection patterns
          #   - Dangerous SQL operations (DROP, TRUNCATE without WHERE)
          #   - Generic security anti-patterns
          # Exits non-zero if any HIGH or CRITICAL findings are found,
          # which will block the pipeline before any upload occurs.
          - script: |
              set -euo pipefail

              SQL_FILES=$(find "$(SCRIPTS_FOLDER)" -name "*.sql" | sort)
              if [ -z "$SQL_FILES" ]; then
                echo "##vso[task.logissue type=warning]No SQL files found — skipping semgrep."
                exit 0
              fi

              echo "Running semgrep security scan on: $(SCRIPTS_FOLDER)/"
              semgrep \
                --config "p/secrets" \
                --config "p/sql-injection" \
                --config "p/security-audit" \
                --severity ERROR \
                --severity WARNING \
                --error \
                --json-output /tmp/semgrep-results.json \
                "$(SCRIPTS_FOLDER)" || SEMGREP_EXIT=$?

              # Always pretty-print findings for the pipeline log.
              if [ -f /tmp/semgrep-results.json ]; then
                FINDING_COUNT=$(python3 -c "
              import json, sys
              d = json.load(open('/tmp/semgrep-results.json'))
              results = d.get('results', [])
              if results:
                  print(f'Found {len(results)} semgrep finding(s):')
                  for r in results:
                      sev  = r['extra']['severity']
                      msg  = r['extra']['message']
                      path = r['path']
                      line = r['start']['line']
                      print(f'  [{sev}] {path}:{line} — {msg}')
              else:
                  print('No semgrep findings.')
              sys.exit(0)
              ")
                echo "$FINDING_COUNT"
              fi

              if [ "${SEMGREP_EXIT:-0}" != "0" ]; then
                echo "##vso[task.logissue type=error]Semgrep found security issues. Fix them before deploying."
                exit 1
              fi

              echo "Semgrep scan passed — no security issues detected."
            displayName: 'Semgrep: security vulnerability scan'
            env:
              SCRIPTS_FOLDER: $(SCRIPTS_FOLDER)

          # ── Step 2: SQLFluff auto-fix (Databricks SparkSQL dialect) ─
          # Automatically corrects:
          #   - Inconsistent casing (keywords, identifiers)
          #   - Trailing whitespace and unnecessary blank lines
          #   - Missing aliases on subqueries
          #   - Implicit column references (SELECT *)
          #   - Indentation and spacing conventions
          # Any files changed by the fix are reported in the log so the
          # developer knows what was sanitised.
          - script: |
              set -euo pipefail

              SQL_FILES=$(find "$(SCRIPTS_FOLDER)" -name "*.sql" | sort)
              if [ -z "$SQL_FILES" ]; then
                echo "##vso[task.logissue type=warning]No SQL files found — skipping sqlfluff fix."
                exit 0
              fi

              echo "Running sqlfluff auto-fix on: $(SCRIPTS_FOLDER)/"
              sqlfluff fix \
                --dialect sparksql \
                --exclude-rules ST06 \
                --force \
                "$(SCRIPTS_FOLDER)" \
                && SQLFLUFF_FIX_EXIT=0 || SQLFLUFF_FIX_EXIT=$?

              # Report which files were changed so the developer is aware.
              CHANGED=$(git diff --name-only -- "$(SCRIPTS_FOLDER)" 2>/dev/null || true)
              if [ -n "$CHANGED" ]; then
                echo "##vso[task.logissue type=warning]sqlfluff auto-fixed the following files:"
                echo "$CHANGED" | while read -r FILE; do
                  echo "  - $FILE"
                done
                echo "--- Diff of sanitised changes ---"
                git diff -- "$(SCRIPTS_FOLDER)" || true
                echo "--- End diff ---"
              else
                echo "No files required auto-fixing."
              fi
            displayName: 'SQLFluff: auto-fix SQL style and best practices'
            env:
              SCRIPTS_FOLDER: $(SCRIPTS_FOLDER)

          # ── Step 3: SQLFluff lint — fail on remaining violations ────
          # Re-lints after the fix pass. If violations remain (i.e. they
          # could not be auto-fixed), the stage fails and blocks deploy.
          # Key Databricks/SparkSQL rules enforced:
          #   L001 Trailing whitespace
          #   L010 Keyword casing (uppercase)
          #   L014 Identifier casing
          #   L019 Comma placement
          #   L034 SELECT wildcards (SELECT * flagged as a warning)
          #   L036 SELECT clause placement
          #   L042 Subquery aliases required
          #   L044 No SELECT * in production queries
          - script: |
              set -euo pipefail

              SQL_FILES=$(find "$(SCRIPTS_FOLDER)" -name "*.sql" | sort)
              if [ -z "$SQL_FILES" ]; then
                echo "##vso[task.logissue type=warning]No SQL files found — skipping sqlfluff lint."
                exit 0
              fi

              echo "Running sqlfluff lint on: $(SCRIPTS_FOLDER)/"
              sqlfluff lint \
                --dialect sparksql \
                --exclude-rules ST06 \
                --format github-annotation \
                "$(SCRIPTS_FOLDER)" \
                && LINT_EXIT=0 || LINT_EXIT=$?

              # Also emit a human-readable summary to the log.
              sqlfluff lint \
                --dialect sparksql \
                --exclude-rules ST06 \
                --format human \
                "$(SCRIPTS_FOLDER)" || true

              if [ "${LINT_EXIT:-0}" != "0" ]; then
                echo "##vso[task.logissue type=error]SQLFluff lint violations remain after auto-fix."
                echo "##vso[task.logissue type=error]Correct the issues listed above, then re-run the pipeline."
                exit 1
              fi

              echo "SQLFluff lint passed — all SQL scripts conform to SparkSQL best practices."
            displayName: 'SQLFluff: lint check (fail on remaining violations)'
            env:
              SCRIPTS_FOLDER: $(SCRIPTS_FOLDER)

  # ------------------------------------------------------------------ #
  # Stage 2: Upload SQL scripts to DBFS                                 #
  # ------------------------------------------------------------------ #
  - stage: Deploy
    displayName: 'Deploy SQL Scripts to DBFS'
    dependsOn: Scan
    jobs:
      - job: UploadScripts
        displayName: 'Upload scripts/ to DBFS'
        steps:

          - checkout: self
            displayName: 'Checkout repository'

          - task: AzureCLI@2
            displayName: 'Upload SQL scripts to DBFS via Databricks REST API'
            inputs:
              azureSubscription: $(AZURE_SERVICE_CONNECTION)
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                set -euo pipefail

                # Acquire an AAD token scoped to the Databricks resource.
                # Resource ID 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d is the
                # well-known Databricks application ID in Azure AD.
                echo "Acquiring AAD token for Databricks using Managed Identity..."
                DATABRICKS_TOKEN=$(az account get-access-token \
                  --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
                  --query accessToken -o tsv)

                # Find all SQL files recursively under the scripts folder.
                SQL_FILES=$(find "$(SCRIPTS_FOLDER)" -name "*.sql" | sort)

                if [ -z "$SQL_FILES" ]; then
                  echo "##vso[task.logissue type=warning]No SQL files found in $(SCRIPTS_FOLDER)/"
                  exit 0
                fi

                for FILE_PATH in $SQL_FILES; do
                  FILE_NAME=$(basename "$FILE_PATH")
                  DBFS_TARGET="$(DBFS_SCRIPTS_PATH)/${FILE_NAME}"

                  echo "Uploading: $FILE_PATH -> $DBFS_TARGET"

                  # Base64-encode the file content (no line wrapping).
                  ENCODED=$(base64 -w 0 "$FILE_PATH")

                  HTTP_STATUS=$(curl -s -o /tmp/dbfs_response.json -w "%{http_code}" \
                    -X POST "$(DATABRICKS_HOST)/api/2.0/dbfs/put" \
                    -H "Authorization: Bearer $DATABRICKS_TOKEN" \
                    -H "Content-Type: application/json" \
                    -d "{
                      \"path\": \"$DBFS_TARGET\",
                      \"overwrite\": true,
                      \"contents\": \"$ENCODED\"
                    }")

                  if [ "$HTTP_STATUS" != "200" ]; then
                    echo "##vso[task.logissue type=error]Failed to upload $FILE_NAME (HTTP $HTTP_STATUS):"
                    cat /tmp/dbfs_response.json
                    exit 1
                  fi

                  echo "Successfully uploaded: $FILE_NAME"
                done

                echo "All SQL scripts uploaded to $(DBFS_SCRIPTS_PATH)."

  # ------------------------------------------------------------------ #
  # Stage 3: Trigger Databricks Job and validate results                #
  # ------------------------------------------------------------------ #
  - stage: Test
    displayName: 'Run Databricks Job and Validate'
    dependsOn: Deploy
    condition: succeeded()
    jobs:
      # ── Job A: Trigger the Databricks run and expose the Run ID ──────── #
      - job: TriggerJob
        displayName: 'Trigger Databricks Job'
        steps:

          - task: AzureCLI@2
            displayName: 'Trigger Databricks Job'
            # 'name' is required so the output variable can be referenced
            # by the downstream MonitorJob via dependencies.<job>.outputs[...].
            name: TriggerStep
            inputs:
              azureSubscription: $(AZURE_SERVICE_CONNECTION)
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                set -euo pipefail

                echo "Acquiring AAD token for Databricks..."
                DATABRICKS_TOKEN=$(az account get-access-token \
                  --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
                  --query accessToken -o tsv)

                echo "Triggering Databricks Job ID: $(DATABRICKS_JOB_ID)..."
                RESPONSE=$(curl -s -X POST "$(DATABRICKS_HOST)/api/2.1/jobs/run-now" \
                  -H "Authorization: Bearer $DATABRICKS_TOKEN" \
                  -H "Content-Type: application/json" \
                  -d "{\"job_id\": $(DATABRICKS_JOB_ID)}")

                RUN_ID=$(echo "$RESPONSE" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d['run_id'])" 2>/dev/null || true)

                if [ -z "$RUN_ID" ]; then
                  echo "##vso[task.logissue type=error]Failed to start Databricks job. Response: $RESPONSE"
                  exit 1
                fi

                echo "Databricks job started. Run ID: $RUN_ID"
                # isOutput=true makes the variable accessible across jobs.
                echo "##vso[task.setvariable variable=DATABRICKS_RUN_ID;isOutput=true]$RUN_ID"

      # ── Job B: Monitor the run using the Databricks CLI ─────────────── #
      - job: MonitorJob
        displayName: 'Monitor Databricks Job via CLI'
        dependsOn: TriggerJob
        variables:
          # Retrieve the Run ID published by TriggerJob above.
          DATABRICKS_RUN_ID: $[ dependencies.TriggerJob.outputs['TriggerStep.DATABRICKS_RUN_ID'] ]
        steps:

          - task: AzureCLI@2
            displayName: 'Install Databricks CLI'
            inputs:
              azureSubscription: $(AZURE_SERVICE_CONNECTION)
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                set -euo pipefail
                # Install the official Databricks CLI (v0.200+).
                curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
                databricks --version

          - task: AzureCLI@2
            displayName: 'Monitor Databricks Job Output via CLI'
            inputs:
              azureSubscription: $(AZURE_SERVICE_CONNECTION)
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                set -euo pipefail

                # Authenticate the CLI using an AAD token for the Databricks resource.
                DATABRICKS_TOKEN=$(az account get-access-token \
                  --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
                  --query accessToken -o tsv)

                export DATABRICKS_HOST="$(DATABRICKS_HOST)"
                export DATABRICKS_TOKEN

                RUN_ID="$(DATABRICKS_RUN_ID)"
                MAX_RETRIES=$(MAX_POLL_RETRIES)
                INTERVAL=$(POLL_INTERVAL_SECONDS)
                RETRY=0

                echo "Monitoring Databricks run $RUN_ID via CLI (max wait: $((MAX_RETRIES * INTERVAL))s)..."

                while [ "$RETRY" -lt "$MAX_RETRIES" ]; do
                  RUN_JSON=$(databricks runs get --run-id "$RUN_ID" --output JSON)

                  LIFE_CYCLE=$(echo "$RUN_JSON" | python3 -c \
                    "import sys,json; print(json.load(sys.stdin)['state']['life_cycle_state'])")

                  echo "  [$((RETRY + 1))/$MAX_RETRIES] Lifecycle: $LIFE_CYCLE"

                  case "$LIFE_CYCLE" in
                    TERMINATED|SKIPPED|INTERNAL_ERROR)
                      RESULT_STATE=$(echo "$RUN_JSON" | python3 -c \
                        "import sys,json; print(json.load(sys.stdin)['state'].get('result_state','UNKNOWN'))")
                      RUN_PAGE_URL=$(echo "$RUN_JSON" | python3 -c \
                        "import sys,json; print(json.load(sys.stdin).get('run_page_url','N/A'))")

                      echo "Job finished. Result: $RESULT_STATE"
                      echo "Run page: $RUN_PAGE_URL"

                      # Stream the task output logs captured by the CLI.
                      echo "--- Task output ---"
                      databricks runs get-output --run-id "$RUN_ID" --output JSON | \
                        python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('logs','(no logs available)'))"

                      if [ "$RESULT_STATE" = "SUCCESS" ]; then
                        echo "##vso[task.complete result=Succeeded;]Databricks job completed successfully."
                        exit 0
                      else
                        echo "##vso[task.logissue type=error]Databricks job failed with result: $RESULT_STATE"
                        echo "##vso[task.logissue type=error]Run page for details: $RUN_PAGE_URL"
                        exit 1
                      fi
                      ;;
                  esac

                  RETRY=$((RETRY + 1))
                  sleep "$INTERVAL"
                done

                echo "##vso[task.logissue type=error]Timed out waiting for Databricks job after $((MAX_RETRIES * INTERVAL)) seconds."
                exit 1
            env:
              DATABRICKS_RUN_ID: $(DATABRICKS_RUN_ID)
              DATABRICKS_MAX_POLL_RETRIES: $(MAX_POLL_RETRIES)
              POLL_INTERVAL_SECONDS: $(POLL_INTERVAL_SECONDS)

  # ------------------------------------------------------------------ #
  # Stage 4: Deploy to TEST environment (main branch only)              #
  # ------------------------------------------------------------------ #
  - stage: DeployToTest
    displayName: 'Deploy to Test Environment (main only)'
    dependsOn: Test
    # Only runs when the previous stages succeeded AND the source branch is main.
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
    jobs:
      # Using a deployment job enables the Azure DevOps Environment with
      # optional approval gates. Create an environment named 'databricks-test'
      # in Azure DevOps (Pipelines > Environments) and add approvers if needed.
      - deployment: DeployTestEnv
        displayName: 'Upload and run scripts on Databricks TEST workspace'
        environment: 'databricks-test'
        strategy:
          runOnce:
            deploy:
              steps:

                - checkout: self
                  displayName: 'Checkout repository'

                - task: AzureCLI@2
                  displayName: '[TEST] Upload SQL scripts to DBFS'
                  inputs:
                    azureSubscription: $(TEST_AZURE_SERVICE_CONNECTION)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      set -euo pipefail

                      echo "Acquiring AAD token for TEST Databricks workspace..."
                      DATABRICKS_TOKEN=$(az account get-access-token \
                        --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
                        --query accessToken -o tsv)

                      SQL_FILES=$(find "$(SCRIPTS_FOLDER)" -name "*.sql" | sort)

                      if [ -z "$SQL_FILES" ]; then
                        echo "##vso[task.logissue type=warning]No SQL files found in $(SCRIPTS_FOLDER)/"
                        exit 0
                      fi

                      for FILE_PATH in $SQL_FILES; do
                        FILE_NAME=$(basename "$FILE_PATH")
                        DBFS_TARGET="$(TEST_DBFS_SCRIPTS_PATH)/${FILE_NAME}"

                        echo "Uploading: $FILE_PATH -> $DBFS_TARGET"

                        ENCODED=$(base64 -w 0 "$FILE_PATH")

                        HTTP_STATUS=$(curl -s -o /tmp/dbfs_response.json -w "%{http_code}" \
                          -X POST "$(TEST_DATABRICKS_HOST)/api/2.0/dbfs/put" \
                          -H "Authorization: Bearer $DATABRICKS_TOKEN" \
                          -H "Content-Type: application/json" \
                          -d "{
                            \"path\": \"$DBFS_TARGET\",
                            \"overwrite\": true,
                            \"contents\": \"$ENCODED\"
                          }")

                        if [ "$HTTP_STATUS" != "200" ]; then
                          echo "##vso[task.logissue type=error]Failed to upload $FILE_NAME (HTTP $HTTP_STATUS):"
                          cat /tmp/dbfs_response.json
                          exit 1
                        fi

                        echo "Successfully uploaded: $FILE_NAME"
                      done

                      echo "All SQL scripts uploaded to $(TEST_DBFS_SCRIPTS_PATH)."

                - task: AzureCLI@2
                  displayName: '[TEST] Trigger Databricks Job'
                  inputs:
                    azureSubscription: $(TEST_AZURE_SERVICE_CONNECTION)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      set -euo pipefail

                      echo "Acquiring AAD token for TEST Databricks workspace..."
                      DATABRICKS_TOKEN=$(az account get-access-token \
                        --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
                        --query accessToken -o tsv)

                      echo "Triggering TEST Databricks Job ID: $(TEST_DATABRICKS_JOB_ID)..."
                      RESPONSE=$(curl -s -X POST "$(TEST_DATABRICKS_HOST)/api/2.1/jobs/run-now" \
                        -H "Authorization: Bearer $DATABRICKS_TOKEN" \
                        -H "Content-Type: application/json" \
                        -d "{\"job_id\": $(TEST_DATABRICKS_JOB_ID)}")

                      RUN_ID=$(echo "$RESPONSE" | python3 -c \
                        "import sys,json; d=json.load(sys.stdin); print(d['run_id'])" 2>/dev/null || true)

                      if [ -z "$RUN_ID" ]; then
                        echo "##vso[task.logissue type=error]Failed to start TEST Databricks job. Response: $RESPONSE"
                        exit 1
                      fi

                      echo "TEST Databricks job started. Run ID: $RUN_ID"
                      echo "##vso[task.setvariable variable=TEST_DATABRICKS_RUN_ID]$RUN_ID"

                - task: AzureCLI@2
                  displayName: '[TEST] Wait for Databricks Job to complete'
                  inputs:
                    azureSubscription: $(TEST_AZURE_SERVICE_CONNECTION)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      set -euo pipefail

                      echo "Acquiring AAD token for TEST Databricks workspace..."
                      DATABRICKS_TOKEN=$(az account get-access-token \
                        --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
                        --query accessToken -o tsv)

                      RUN_ID="$TEST_DATABRICKS_RUN_ID"
                      MAX_RETRIES=$(DATABRICKS_MAX_POLL_RETRIES)
                      INTERVAL=$(POLL_INTERVAL_SECONDS)
                      RETRY=0

                      echo "Polling TEST run $RUN_ID (max wait: $((MAX_RETRIES * INTERVAL))s)..."

                      while [ "$RETRY" -lt "$MAX_RETRIES" ]; do
                        RESULT=$(curl -s -X GET \
                          "$(TEST_DATABRICKS_HOST)/api/2.1/jobs/runs/get?run_id=${RUN_ID}" \
                          -H "Authorization: Bearer $DATABRICKS_TOKEN")

                        LIFE_CYCLE=$(echo "$RESULT" | python3 -c \
                          "import sys,json; print(json.load(sys.stdin)['state']['life_cycle_state'])")

                        echo "  [$((RETRY + 1))/$MAX_RETRIES] Lifecycle state: $LIFE_CYCLE"

                        case "$LIFE_CYCLE" in
                          TERMINATED|SKIPPED|INTERNAL_ERROR)
                            RESULT_STATE=$(echo "$RESULT" | python3 -c \
                              "import sys,json; print(json.load(sys.stdin)['state'].get('result_state','UNKNOWN'))")
                            RUN_PAGE_URL=$(echo "$RESULT" | python3 -c \
                              "import sys,json; print(json.load(sys.stdin).get('run_page_url','N/A'))")

                            echo "TEST Job finished. Result: $RESULT_STATE"
                            echo "Run page: $RUN_PAGE_URL"

                            if [ "$RESULT_STATE" = "SUCCESS" ]; then
                              echo "##vso[task.complete result=Succeeded;]TEST Databricks job completed successfully."
                              exit 0
                            else
                              echo "##vso[task.logissue type=error]TEST Databricks job failed with result: $RESULT_STATE"
                              echo "##vso[task.logissue type=error]Run page for details: $RUN_PAGE_URL"
                              exit 1
                            fi
                            ;;
                        esac

                        RETRY=$((RETRY + 1))
                        sleep "$INTERVAL"
                      done

                      echo "##vso[task.logissue type=error]Timed out waiting for TEST Databricks job after $((MAX_RETRIES * INTERVAL)) seconds."
                      exit 1
                  env:
                    TEST_DATABRICKS_RUN_ID: $(TEST_DATABRICKS_RUN_ID)
                    DATABRICKS_MAX_POLL_RETRIES: $(MAX_POLL_RETRIES)

  # ------------------------------------------------------------------ #
  # Stage 5: Slack notification (always runs, success or failure)       #
  # ------------------------------------------------------------------ #
  - stage: Notify
    displayName: 'Notify Team on Slack'
    # Always run this stage so the team is notified on both success and failure.
    dependsOn:
      - Scan
      - Deploy
      - Test
      - DeployToTest
    condition: always()
    jobs:
      - job: SlackNotify
        displayName: 'Post build result to Slack'
        steps:
          - task: Bash@3
            displayName: 'Send Slack notification'
            condition: always()
            inputs:
              targetType: 'inline'
              script: |
                set -euo pipefail

                # ── Determine overall pipeline result ──────────────────────────
                # Stage results are injected via the env: block below using
                # stageDependencies expressions. We check each stage in turn.

                # Read individual stage outcomes.
                STAGE_SCAN="$DEPLOY_RESULT_STAGE_SCAN"
                STAGE_DEPLOY="$DEPLOY_RESULT_STAGE_DEPLOY"
                STAGE_TEST="$DEPLOY_RESULT_STAGE_TEST"
                STAGE_TEST_ENV="$DEPLOY_RESULT_STAGE_DEPLOYTOTEST"

                # Determine the highest-severity outcome across all stages.
                OVERALL="Succeeded"
                FAILED_STAGES=""

                for STAGE_INFO in "Scan:$STAGE_SCAN" "Deploy:$STAGE_DEPLOY" "Test:$STAGE_TEST" "DeployToTest:$STAGE_TEST_ENV"; do
                  STAGE_NAME="${STAGE_INFO%%:*}"
                  STAGE_STATUS="${STAGE_INFO##*:}"
                  case "$STAGE_STATUS" in
                    Failed|Canceled)
                      OVERALL="Failed"
                      FAILED_STAGES="${FAILED_STAGES}• *${STAGE_NAME}* — ${STAGE_STATUS}\n"
                      ;;
                    Skipped)
                      # Skipped stages are expected (e.g. DeployToTest on non-main).
                      ;;
                  esac
                done

                # ── Build message fields ───────────────────────────────────────
                PIPELINE_URL="$PIPELINE_URL"
                BRANCH="${BRANCH//refs\/heads\//}"
                COMMIT="${COMMIT:0:8}"
                TRIGGERED_BY="$TRIGGERED_BY"

                if [ "$OVERALL" = "Succeeded" ]; then
                  COLOUR="good"       # Green in Slack
                  ICON=":white_check_mark:"
                  TITLE="Databricks deployment succeeded"
                  DETAIL="All stages completed successfully."
                else
                  COLOUR="danger"     # Red in Slack
                  ICON=":x:"
                  TITLE="Databricks deployment FAILED"
                  # Trim trailing newline from failed stages list.
                  DETAIL="Failed stages:\n${FAILED_STAGES}\nCheck the pipeline logs for full details."
                fi

                # Export computed values so the Python heredoc can read them.
                export COLOUR ICON TITLE DETAIL PIPELINE_URL BRANCH COMMIT TRIGGERED_BY
                TIMESTAMP=$(date -u '+%Y-%m-%d %H:%M UTC')
                export TIMESTAMP

                # ── Build Slack Block Kit payload ──────────────────────────────
                PAYLOAD=$(python3 - <<PYEOF
                import json, os

                colour   = os.environ.get('COLOUR', 'good')
                icon     = os.environ.get('ICON', ':white_check_mark:')
                title    = os.environ.get('TITLE', 'Databricks deployment result')
                detail   = os.environ.get('DETAIL', '')
                url      = os.environ.get('PIPELINE_URL', '')
                branch   = os.environ.get('BRANCH', 'unknown')
                commit   = os.environ.get('COMMIT', 'unknown')
                by       = os.environ.get('TRIGGERED_BY', 'unknown')
                ts       = os.environ.get('TIMESTAMP', '')
                channel  = os.environ.get('SLACK_CHANNEL', '#databricks-deployments')

                payload = {
                  "channel": channel,
                  "attachments": [
                    {
                      "color": colour,
                      "blocks": [
                        {
                          "type": "header",
                          "text": {"type": "plain_text", "text": f"{icon}  {title}", "emoji": True}
                        },
                        {
                          "type": "section",
                          "text": {"type": "mrkdwn", "text": detail.replace("\\n", "\n")}
                        },
                        {"type": "divider"},
                        {
                          "type": "section",
                          "fields": [
                            {"type": "mrkdwn", "text": f"*Branch:*\n`{branch}`"},
                            {"type": "mrkdwn", "text": f"*Commit:*\n`{commit}`"},
                            {"type": "mrkdwn", "text": f"*Triggered by:*\n{by}"},
                            {"type": "mrkdwn", "text": f"*Time:*\n{ts}"}
                          ]
                        },
                        {
                          "type": "actions",
                          "elements": [
                            {
                              "type": "button",
                              "text": {"type": "plain_text", "text": "View Pipeline Logs", "emoji": True},
                              "url": url,
                              "style": "primary" if colour == "good" else "danger"
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
                print(json.dumps(payload))
                PYEOF
                )

                # ── Post to Slack ──────────────────────────────────────────────
                echo "Sending Slack notification (overall: $OVERALL)..."

                HTTP_STATUS=$(curl -s -o /tmp/slack_response.json -w "%{http_code}" \
                  -X POST "$SLACK_WEBHOOK_URL" \
                  -H "Content-Type: application/json" \
                  --data "$PAYLOAD")

                if [ "$HTTP_STATUS" != "200" ]; then
                  echo "##vso[task.logissue type=warning]Slack notification failed (HTTP $HTTP_STATUS):"
                  cat /tmp/slack_response.json
                else
                  echo "Slack notification sent successfully."
                fi
            env:
              # Secret webhook URL — store this in Azure DevOps pipeline variables (secret).
              SLACK_WEBHOOK_URL: $(SLACK_WEBHOOK_URL)
              SLACK_CHANNEL: $(SLACK_CHANNEL)
              # ADO build metadata passed into the Python script.
              PIPELINE_URL: $(System.TeamFoundationServerUri)$(System.TeamProject)/_build/results?buildId=$(Build.BuildId)
              BRANCH: $(Build.SourceBranch)
              COMMIT: $(Build.SourceVersion)
              TRIGGERED_BY: $(Build.RequestedForEmail)
              # Stage result variables — injected by ADO because the stages are listed in dependsOn.
              DEPLOY_RESULT_STAGE_SCAN: $[ stageDependencies.Scan.result ]
              DEPLOY_RESULT_STAGE_DEPLOY: $[ stageDependencies.Deploy.result ]
              DEPLOY_RESULT_STAGE_TEST: $[ stageDependencies.Test.result ]
              DEPLOY_RESULT_STAGE_DEPLOYTOTEST: $[ stageDependencies.DeployToTest.result ]
